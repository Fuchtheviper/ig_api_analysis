{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import logging\n",
    "from pymongo.errors import BulkWriteError\n",
    "import ollama\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant\n",
    "INSTAGRAM_API_URL = \"https://instagram-scraper-api2.p.rapidapi.com/v1/hashtag\"\n",
    "HEADERS = {\n",
    "    \"x-rapidapi-key\": \"YOUR_RAPIDAPI_KEY\",\n",
    "    \"x-rapidapi-host\": \"instagram-scraper-api2.p.rapidapi.com\"\n",
    "}\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "DB_NAME = \"ig_post\"\n",
    "COLLECTION_NAME = \"instagram_posts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Fetch Data for One Hashtag\n",
    "def fetch_all_data_for_hashtag(hashtag):\n",
    "    all_items = []\n",
    "    querystring = {\"hashtag\": hashtag}\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers, params=querystring)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching {hashtag}: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        items = data.get(\"data\", {}).get(\"items\", [])\n",
    "        all_items.extend(items)\n",
    "        \n",
    "        # Check if there is a next page\n",
    "        next_page = data.get(\"data\", {}).get(\"next_page\")\n",
    "        if not next_page:\n",
    "            break  # Stop if no more pages\n",
    "        \n",
    "        querystring[\"next_page\"] = next_page  # Use next page token\n",
    "        time.sleep(1)  # Avoid hitting API rate limits\n",
    "\n",
    "    return all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Extract Insights\n",
    "def extract_insights(items, hashtag):\n",
    "    insights = []\n",
    "    for item in items:\n",
    "        caption_text = item.get(\"caption\", {}).get(\"text\", \"\").lower()\n",
    "        \n",
    "        # Extract hashtags from text using regex\n",
    "        hashtags = re.findall(r\"#(\\w+)\", caption_text)\n",
    "\n",
    "        # Extract additional details\n",
    "        post_id = item.get(\"id\", None)\n",
    "        comment_count = item.get(\"comment_count\", 0)\n",
    "        feed_type = item.get(\"feed_type\", \"\")\n",
    "        is_video = item.get(\"is_video\", False)\n",
    "        like_count = item.get(\"like_count\", 0)\n",
    "        media_name = item.get(\"media_name\", \"\")\n",
    "        product_type = item.get(\"product_type\", \"\")\n",
    "        video_duration = item.get(\"video_duration\", 0.0)\n",
    "\n",
    "        # Check for mentions of the hashtag in text or hashtags\n",
    "        if hashtag in caption_text or any(tag.lower() == hashtag for tag in hashtags):\n",
    "            insights.append({\n",
    "                \"text\": caption_text,\n",
    "                \"hashtags\": hashtags,\n",
    "                \"id\": post_id,\n",
    "                \"comment_count\": comment_count,\n",
    "                \"feed_type\": feed_type,\n",
    "                \"is_video\": is_video,\n",
    "                \"like_count\": like_count,\n",
    "                \"media_name\": media_name,\n",
    "                \"product_type\": product_type,\n",
    "                \"video_duration\": video_duration,\n",
    "            })\n",
    "    \n",
    "    return insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Data for Both Hashtags\n",
    "hashtags = [\"thailand\", \"bangkok\",\"ไทย\", \"กรุงเทพ\", \"bkk\", \"bangkokcity\", \"thai\", \"bangkokthailand\", \"amazingthailand\", \"y2kthailand\",\"thailandtravel\"]\n",
    "all_insights = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Fetch Data from all hashtag\n",
    "for hashtag in hashtags:\n",
    "    print(f\"Fetching data for #{hashtag}...\")\n",
    "    data_items = fetch_all_data_for_hashtag(hashtag)\n",
    "    insights = extract_insights(data_items, hashtag)\n",
    "    all_insights[hashtag] = insights\n",
    "    print(f\"Total posts retrieved for #{hashtag}: {len(insights)}\\n\")\n",
    "\n",
    "# Sum total post getting from API\n",
    "total_posts = sum(len(posts) for posts in all_insights.values())\n",
    "print(f\"Total number of relevant posts: {total_posts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install pymongo for notebook\n",
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save raw data to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymongo.errors import BulkWriteError\n",
    "# Assuming all_insights is a dictionary with hashtag keys and list of insights as values.\n",
    "# Flatten the insights into a single list.\n",
    "all_data = [post for insights in all_insights.values() for post in insights]\n",
    "for doc in all_data:\n",
    "    doc.pop(\"_id\", None)\n",
    "\n",
    "logging.info(f\"Attempting to insert {len(all_data)} documents into MongoDB.\")\n",
    "\n",
    "try:\n",
    "    for doc in all_data:\n",
    "        filter_query = {\"id\": doc[\"id\"]}  # Adjust based on your unique field\n",
    "        update_query = {\"$set\": doc}\n",
    "        collection.update_one(filter_query, update_query, upsert=True)\n",
    "\n",
    "    logging.info(\"All new data upserted into MongoDB!\")\n",
    "except BulkWriteError as bwe:\n",
    "    logging.error(\"Bulk write error occurred during update operation.\")\n",
    "    logging.error(bwe.details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from MongoDB\n",
    "data = list(collection.find({}, {\"text\": 1}))\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text\n",
    "def clean_text(text):\n",
    "    if text:\n",
    "        text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)  # Remove URLs\n",
    "        text = re.sub(r\"#\\w+\", \"\", text)  # Remove hashtags\n",
    "        text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n",
    "        return text.strip()\n",
    "    return \"\"\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API Key from .env file\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Use ChatGPT for Sentiment Analysis\n",
    "def get_sentiment_with_chatgpt(text):\n",
    "    if text:\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo-1106\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a sentiment analysis assistant. Classify the sentiment of the given text.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Classify this Instagram caption into one of these categories: Positive, Negative, Neutral:\\n\\n{text}\"}\n",
    "                ],\n",
    "                temperature=0.1  # Low temperature for deterministic response\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenAI API: {e}\")\n",
    "            return \"Neutral\"  # Default fallback\n",
    "    return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Sentiment Analysis using ChatGPT API\n",
    "df[\"sentiment\"] = df[\"clean_text\"].apply(get_sentiment_with_chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure Data quality from response\n",
    "def extract_sentiment(text):\n",
    "    # Define the list of sentiment words to look for.\n",
    "    sentiments = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "    \n",
    "    # Convert the input text to lowercase for case-insensitive comparison.\n",
    "    lower_text = text.lower()\n",
    "    \n",
    "    # Check each sentiment word.\n",
    "    for sentiment in sentiments:\n",
    "        if sentiment.lower() in lower_text:\n",
    "            return sentiment  # Return the sentiment in its original form.\n",
    "    \n",
    "    # Return None if no sentiment word is found.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text cleaning\n",
    "df[\"sentiment\"] = df[\"sentiment\"].apply(extract_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Processed Data Back in MongoDB\n",
    "for index, row in df.iterrows():\n",
    "    collection.update_one({\"text\": row[\"text\"]}, {\"$set\": {\"sentiment\": row[\"sentiment\"]}}, upsert=True)\n",
    "\n",
    "print(\"Sentiment Analysis Completed using ChatGPT API!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Analysis with Local run DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Use Deepseek for Topic Analysis\n",
    "def get_topic_with_deepseek(text):\n",
    "    if text:\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=\"deepseek-v2:16b\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a topic analysis assistant. Classify the topic of the given text.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Analyze the given multi-language Instagram caption and classify it into only one topic. The topic should be specific enough to provide meaningful insights but not overly niche. If multiple topics are highly related, consolidate them into a common broader category instead of listing them separately. Return only one word representing the topic in English. If the caption cannot be analyzed, return only the word Unknown (without quotes or additional explanation).:\\n\\n{text}\"}\n",
    "                ],\n",
    "                options={\"temperature\": 0}\n",
    "            )\n",
    "            time.sleep(0.5)  # Avoid overloading Ollama with fast requests\n",
    "            return response[\"message\"][\"content\"] if \"message\" in response else \"Unknown\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling DeepSeek: {e}\")\n",
    "            return \"Unknown\"  # Default fallback in case of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Topic Analysis\n",
    "batch_size = 10  # Adjust batch size based on available memory\n",
    "num_batches = len(df) // batch_size + 1\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(df))\n",
    "    \n",
    "    df.loc[start_idx:end_idx, \"topic\"] = df.loc[start_idx:end_idx, \"clean_text\"].apply(get_topic_with_deepseek)\n",
    "\n",
    "    print(f\"Processed batch {i+1}/{num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Processed Data Back in MongoDB\n",
    "for index, row in df.iterrows():\n",
    "    collection.update_one({\"text\": row[\"text\"]}, {\"$set\": {\"topic\": row[\"topic\"]}}, upsert=True)\n",
    "\n",
    "print(\"Topic Analysis Completed using Local DeepSeek!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from MongoDB\n",
    "df = pd.DataFrame(list(collection.find({}, {\"_id\": 1, \"topic\": 1})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regex pattern for detecting emojis\n",
    "emoji_regex = re.compile(\n",
    "    r\"[\\U0001F300-\\U0001F5FF\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF\"\n",
    "    r\"\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\"\n",
    "    r\"\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\"\n",
    "    r\"\\U00002702-\\U000027B0]+\", flags=re.UNICODE\n",
    ")\n",
    "\n",
    "# Filter DataFrame where \"topic\" contains an emoji\n",
    "df_with_emoji = df[df[\"topic\"].str.contains(emoji_regex, na=False)]\n",
    "print(len(df_with_emoji))\n",
    "\n",
    "# Replace data with emoji with Unknown\n",
    "df.loc[df[\"topic\"].str.contains(emoji_regex, na=False), \"topic\"] = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check text length and replace topic with \"Unknown\" if over 50 characters\n",
    "print(len(df[\"topic\"].str.len() > 50))\n",
    "df.loc[df[\"topic\"].str.len() > 50, \"topic\"] = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update MongoDB\n",
    "for _, row in df.iterrows():\n",
    "    collection.update_one({\"_id\": row[\"_id\"]}, {\"$set\": {\"topic\": row[\"topic\"]}})\n",
    "\n",
    "print(\"Updated topics in MongoDB for long texts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate non-eng with GoogleTrans API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize translator\n",
    "translator = Translator()\n",
    "\n",
    "# Function to translate only non-English text\n",
    "def translate_if_needed(text):\n",
    "    try:\n",
    "        detected_lang = translator.detect(text).lang  # No async\n",
    "        if detected_lang == \"en\":\n",
    "            return text\n",
    "        return translator.translate(text, dest=\"en\").text  # No await needed\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Translation Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply translation function\n",
    "df[\"topic\"] = df[\"topic\"].apply(translate_if_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update MongoDB\n",
    "for _, row in df.iterrows():\n",
    "    collection.update_one({\"_id\": row[\"_id\"]}, {\"$set\": {\"topic\": row[\"topic\"]}})\n",
    "\n",
    "print(\"Updated topics in MongoDB for non eng to eng\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
